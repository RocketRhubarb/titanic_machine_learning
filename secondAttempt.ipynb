{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the Keggle kernel https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Packages\n",
    "'''\n",
    "\n",
    "import sys # access to system parameters\n",
    "print('Python version: {}'.format(sys.version))\n",
    "\n",
    "import pandas as pd # library of functions for data processing\n",
    "print('pandas version: {}'.format(pd.__version__))\n",
    "\n",
    "import matplotlib # functions for visualization\n",
    "print('matplotlib version: {}'.format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np # package for scientific computing\n",
    "print('NumPy version: {}'.format(np.__version__))\n",
    "\n",
    "import scipy as sp # functions for scientific computing and advanced mathematics\n",
    "print('SciPy version: {}'.format(sp.__version__))\n",
    "\n",
    "import IPython.display\n",
    "from IPython import display # printing fancy in Jupyter notebook\n",
    "print('IPython version: {}'.format(IPython.__version__))\n",
    "\n",
    "import sklearn # collection of machine learning algorithms\n",
    "print('scikit-learn version: {}'.format(sklearn.__version__))\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#print('-'*25)\n",
    "\n",
    "# Input datafiles\n",
    "#from subprocess import check_output\n",
    "#print(check_output(['ls','/data']).decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data modelling libraries\n",
    "'''\n",
    "# Model algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "#from xgboost import XGBClassifier # only works on osx and linux\n",
    "\n",
    "# Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Configure visualization defaults\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and investigate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First look at the data\n",
    "'''\n",
    "\n",
    "# data for training and testing\n",
    "data_raw = pd.read_csv('data/train.csv') \n",
    "\n",
    "# final validation set for competition submission\n",
    "data_val = pd.read_csv('data/test.csv') \n",
    "\n",
    "# create a copy of the data to use\n",
    "data1 = data_raw.copy(deep=True)\n",
    "\n",
    "# reference for convenience. Helpful for cleaning operations\n",
    "data_cleaner = [data1, data_val]\n",
    "\n",
    "print(data_raw.info())\n",
    "print(data_val.info())\n",
    "#data_raw.head()\n",
    "#data_raw.tail()\n",
    "#data_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print('-'*25)\n",
    "\n",
    "print('Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print('-'*25)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning: Correcting, Completing, Creating and Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for correction (no misstakes or extreme outliers)\n",
    "\n",
    "# Completing\n",
    "for dataset in data_cleaner: # apply to both training and validation set\n",
    "    # complete missing age with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(),inplace=True)\n",
    "    \n",
    "    # complete embark with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0],inplace=True)\n",
    "    \n",
    "    # complete missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(),inplace=True)\n",
    "    \n",
    "# delete the features to exclude in training dataset\n",
    "drop_column = ['PassengerId','Cabin','Ticket']\n",
    "data1.drop(drop_column,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have we completed?\n",
    "print(data1.isnull().sum())\n",
    "print('-'*25)\n",
    "print(data_val.isnull().sum())\n",
    "# Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create: feature engineering from train and validation dataset\n",
    "for dataset in data_cleaner:\n",
    "    # Discrete variables\n",
    "    \n",
    "    # Size of families\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    \n",
    "    # Travelin alone\n",
    "    dataset['IsAlone'] = 1\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize']>1] = 0\n",
    "    \n",
    "    # Extract title from name\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \",expand=True)[1].str.split(\".\",expand=True)[0]\n",
    "    \n",
    "    # Continous variables\n",
    "    \n",
    "    # Bin Fares in quantiles\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'],4)\n",
    "    \n",
    "    # Bin Ages in bins of equal width\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int),5)\n",
    "    \n",
    "# clean rare title names\n",
    "stat_min = 10\n",
    "title_names_data1 = (data1['Title'].value_counts() < stat_min)\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names_data1.loc[x] == True else x)\n",
    "\n",
    "title_names_data_val = (data_val['Title'].value_counts() < stat_min)\n",
    "data_val['Title'] = data_val['Title'].apply(lambda x: 'Misc' if title_names_data_val.loc[x] == True else x)\n",
    "\n",
    "# Preview data\n",
    "#print(data1['Title'].value_counts())\n",
    "#print(data_val['Title'].value_counts())\n",
    "#data1.info()\n",
    "#data_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting formats: Dummy variables for mathematical analysis using Label Encoder\n",
    "\n",
    "# Code categorical data\n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    dataset['Sex_code'] = LabelEncoder().fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_code'] = LabelEncoder().fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_code'] = LabelEncoder().fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_code'] = LabelEncoder().fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_code'] = LabelEncoder().fit_transform(dataset['FareBin'])\n",
    "\n",
    "# define yvariable aka Target/outcome \n",
    "Target = ['Survived']\n",
    "\n",
    "data1_x = [\n",
    "    'Sex','Pclass', 'Embarked', \n",
    "    'Title', 'SibSp', 'Parch', \n",
    "    'Age', 'Fare', 'FamilySize', \n",
    "    'IsAlone'\n",
    "]\n",
    "\n",
    "data1_x_calc = [\n",
    "    'Sex_code','Pclass', 'Embarked_code', \n",
    "    'Title_code', 'SibSp', 'Parch', \n",
    "    'Age', 'Fare', 'FamilySize', \n",
    "    'IsAlone'\n",
    "]\n",
    "\n",
    "data1_x_bin = [\n",
    "    'Sex_code','Pclass', 'Embarked_code', \n",
    "    'Title_code', 'FamilySize', 'AgeBin_code',\n",
    "    'FareBin_code'\n",
    "]\n",
    "\n",
    "data1_xy = Target + data1_x\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "# Define x and y variables for dummy feature original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print('-'*25)\n",
    "print(data1.info())\n",
    "print('-'*25)\n",
    "\n",
    "print('Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print('-'*25)\n",
    "print(data_val.info())\n",
    "print('-'*25)\n",
    "\n",
    "data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
